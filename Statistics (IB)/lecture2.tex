
\begin{example}
Suppose $X_1, \dotsc, X_n$ are i.i.d. $\Normal(\mu, 1)$ ($\mu$ unknown). A possible estimator estimator for $\mu$ is the sample mean:
\[
 T\left(\vec{X}\right) = \frac{1}{n}\sum_{i=1}^n X_i \mpunct{.}
\]
The sampling distribution of $T\left(\vec{x}\right)$ is $\Normal\left(\mu, \frac{1}{n}\right)$, hence $\EE_\mu \left[ T\left(\vec{x}\right) \right] = \mu$, for all $\mu \in \RR$.
\end{example}

\section{Sufficiency}
The concept of sufficiency addresses the question ``is there a statistic $T\left(\vec{X}\right)$ that in some sense captures all the information about $\theta$ contained in the sample?''

\begin{example}[label=example:1.2]
Let $X_1, \dotsc, X_n$ \iid $\Bernouilli(\theta)$, so that $\PP(X_i = 1) = 1 - \PP(X_i = 0) = \theta$, for some $\theta$, $0 < \theta < 1$. The joint \pmf of $\vec{X}$ is:
\[
p_{\vec{X}}\left(\vec{x}; \theta\right) = \prod_{i = 1}^n \theta^{x_i}(1-\theta)^{1-x_i} = \theta^{\sum x_i}(1 - \theta)^{n - \sum x_i} \mpunct{.}
\]
This depends on the data only through $T\left(\vec{x}\right) = \sum_{i=1}^n$ (the total number of $1$). The conditional \pmf of $\vec{X} \given T$ if $T\left(\vec{x}\right) = t$ is:
\[
p_{\vec{X} \given T} \left( \vec{x} \given t\right) = \frac{\PP_\theta(\vec{X} = x, T = t)}{\PP_\theta\left(T = t\right)} = \frac{\theta^t(1-\theta)^{n-t}}{\binom{n}{t}\theta^t(1-\theta)^{n-t}} = \frac{1}{\binom{n}{t}} \mpunct{.}
\]
i.e. the conditional distribution of $\vec{X}$ given $T = t$ does not depend on $\theta$. Thus, if we know $T$, then additional knowledge of $\vec{X}$ (e.g. knowing the exact sequence of $0$ and $1$) does not give extra information about $\theta$.
\end{example}

\begin{definition}
  A statistic $T$ is sufficient\index{sufficient} for $\theta$ if the conditional distribution, of $\vec{X}$ given $T$ does not depend on $\theta$.
\end{definition}

In the previous example, $T\left(\vec{X}\right) = \sum_{i=1}^n X_i$ is sufficient for $\theta$.

In practice, sufficient statistics are found using the following theorem.

\begin{theorem}[Factorisation criterion\index{Factorisation criterion}]
  \label{thm:factorisation_criterion}
  $T$ is sufficient for $\theta$ if and only if there exist functions $g$ and $h$ such that:
\[
f_{\vec{X}} \left(\vec{x}; \theta\right) = g\left( T\left(\vec{x}\right), \theta\right)h\left(\vec{x}\right)
\]
\end{theorem}

\begin{proof}
  We prove the discrete case.

Suppose that $p_{\vec{X}} \left(\vec{x}; \theta\right) = g\left(T\left(\vec{x}\right), \theta \right) h\left(\vec{x}\right)$. If $T\left(\vec{x}\right) = t$, then we have:
\begin{IEEEeqnarray*}{rCl}
p_{\vec{X} \given T} \left (\vec{x} \given t \right) &=& \frac{\PP_\theta \left( \vec{X} = \vec{x}, T\left(\vec{X}\right) = t\right)}{\PP_\theta \left(T = t\right)} \\
&=& \frac{g\left(T\left(\vec{x}\right), \theta\right)h\left(\vec{x}\right)}{\sum_{\{\vec{x} \mid T\left(\vec{x'}\right) = t\}}g\left(T\left(\vec{x'}\right), \theta\right)h\left(\vec{x'}\right)} \\
&=& \frac{g(t, \theta)h(\vec{x})}{g(t, \theta)\sum_{\{\vec{x} \mid T\left(\vec{x'}\right) =t\}} h(\vec{x'})} \\
&=& \frac{h(\vec{x})}{\sum_{\{\vec{x} \mid T\left(\vec{x'}\right) = t\}} h(\vec{x'})} \mpunct{.}
\end{IEEEeqnarray*}
This does not depend on $\theta$, hence $T$ is sufficient for $\theta$.

Conversely, suppose $T$ is sufficient for $\theta$. If $T\left(\vec{x}\right) = t$, then
\begin{IEEEeqnarray*}{rCl}
\PP_\theta \left(\vec{X} = \vec{x}\right)  \\
&=& \sum_{t'} \underbrace{\PP_\theta \left(\vec{X} = \vec{x}\right)}_{\text{for this $x$, this probability is zero unless $t' = t$}}\PP_\theta\left(T = t'\right)  \\
&=& \underbrace{\PP_\theta \left(\vec{X} = \vec{x} \given T = t\right)}_{\text{independent of $\theta$ by assumption of sufficiency}}\PP_\theta(T = t)
\end{IEEEeqnarray*}
\end{proof}

\begin{example}
Consider \vref{example:1.2}, take $g(t, \theta) = \theta^t(1-\theta)^{n-t}$ and $h(\vec{x}) = 1$, we then see that $T(\vec{X}) = \sum X_i$ is sufficient for $\theta$ by \nameref{thm:factorisation_criterion}.

Now consider $X_1, \dotsc, X_n$ \iid $\Uniform [0, \theta]$. Write $1\left(A\right)$ for the indicator of $A$. Then we have:
\begin{IEEEeqnarray*}{rCl}
  f_{\vec{X}}(\vec{x}; \theta) &=& \prod_{i=1}^n \frac{1}{\theta} \indicator{0 \leq x_i \leq \theta} \\
&=& \frac{1}{\theta^n}\indicator{\max_i x_i \leq \theta}\indicator{\min_i x_i \geq 0} \mpunct{.}
\end{IEEEeqnarray*}
By the \nameref{thm:factorisation_criterion}, $T(\vec{X}) = \max_i X_i$ is sufficient for $\theta$. For example, consider:
\[
g(t, \theta) = \frac{1}{\theta^n}\indicator{t \leq \theta} \text{ and } h(\vec{x}) = \indicator{\min_i x_i \leq 0} \mpunct{.}
\]
\end{example}

Sufficient statistics are not unique: if $T$ is sufficient, then so is any bijective function of $T$. The complete sample $\vec{X}$ is always sufficient for $\theta$: let $T\left(\vec{X}\right)$, $g\left(\vec{t}, \theta\right) = f_{\vec{X}}(\vec{x}, \theta)$ and $h(\vec{x}) = 1$.

The sample space $\curlyX^n$ is partitioned by $T$ into sets
\[
\left\{ \vec{x} \in \curlyX^n \mid T\left(\vec{x}\right) = t \right\} \mpunct{,}
\]
and if $T$ is sufficient, then this data reduction does not lose an information on $\theta$.
We now seek a statistic that achieves the maximum possible data reduction.

\begin{definition}
  A sufficient statistic $T(\vec{x})$ is \emph{minimal sufficient\index{minimal sufficient}} if it is a function of every other sufficient statistic, i.e. if $T'(\vec{x})$ is also sufficient, then
\[
T'(\vec{x}) = T'(\vec{y}) \Rightarrow T(\vec{x}) = T(\vec{y}) \mpunct{.}
\]
Equivalently, the partition for $T$ is coarser than that for $T'$ (i.e. if $A_{T' = t'}$ is a partition set for $T'$, then $A_{T' = t'} \subseteq A_{T = t}$ for some $t$).
\end{definition}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "statistics"
%%% End:
