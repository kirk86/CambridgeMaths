
\chapter{Hypothesis testing}

\section{Introduction}

Let $X_1, \dotsc, X_n$ be \iid random variables, each taking value in $\curlyX$ with unknown probability distribution/mass function $f$.
In hypothesis testing\index{hypothesis testing}, based on data $\vec{X} = \vec{x}$, we make a choice between two hypotheses, $H_0$ and $H_1$, about $f$.

\paragraph{Examples}
\begin{itemize}
\item $X_i \sim \Bernouilli(\theta)$, $H_0 : \theta = 1/2$ versus $H_1 : \theta = 3/4$.
\item $X_i \sim \Bernouilli(\theta)$, $H_0 : \theta = 1/2$ versus $H_1 : \theta \neq 1/2$.
\item $X_i \sim \Bernouilli(\theta)$, $H_0 \leq 1/2$ versus $H_1 : \theta > 1/2$.
\item $H_0 : f = f_0$ versus $H_1 : f = f_1$ where $f_0$ and $f_1$ are completely specified but could be from different parametric families.
\end{itemize}

A simple hypothesis\index{simple hypothesis} $H$ specifies $f$ completely (e.g. $H_0 : \theta = 1/2$, $H_1 : f = f_1$, \dots); otherwise $H$ is composite\index{composite (hypothesis)} (e.g. $H_1 : \theta \neq 1/2$, $H_0 : \theta \leq 1/2$, \dots).

A hypothesis test\index{hypothesis test} partitions $\curlyX^n$ into two regions $C$ and $\overline{C}$ (complement of $C$) such that if $\vec{x} \in C$ then $H_0$ is rejected, and if $\vec{x} \in \overline{C}$ then $H_0$ is not rejected.
The critical region\index{critical region} (or rejection region\index{rejection region}) $C$ defines the test.
Alternatively, a test may be defined by a function $\varphi$ on $\curlyX^n$ such that $\varphi(\vec{x}) = 1$ if $\vec{x} \in C$ and $\varphi(\vec{x}) = 0$ otherwise.

If we carry out a hypothesis test, we either reach the correct conclusion, or we make one of two possible types of error:
\begin{enumerate}
\item reject $H_0$ when it is true (Type I error\index{Type I error}),
\item do not reject $H_0$ when it is false (Type II error\index{Type II error}).
\end{enumerate}

\section{Testing simple hypotheses}

Suppose $H_0$ and $H_1$ are both simple hypotheses.
Let $\alpha = \PP(\text{Type I error}) = \PP(\vec{X} \in C \given H_0)$ be the size\index{size} of the test.
Let $\beta = \PP(\text{Type II error}) = \PP(\vec{X} \not\in C \given H_1)$.
The power\index{power (test)} of the test is $\PP(\vec{X} \in C \given H_0 \text{ false}) = 1 - \beta$.

The likelihood\index{likelihood (hypothesis)} of a simple hypothesis is defined as
\[
L_{\vec{x}} (H) = f_\vec{X} \left( \vec{x} \given H \right) \mpunct{.}
\]
The likelihood ratio\index{likelihood ratio} of two simple hypotheses $H_0$ and $H_1$ is defined as
\[
\Lambda_{\vec{x}} (H_0; H_1) = \frac{L_{\vec{x}}(H_1)}{L_{\vec{x}}(H_0)} \mpunct{.}
\]
A likelihood ratio test\index{likelihood ratio test}\index{LR test} (LR test) is one with critical region $C$ of the form
\[
C = \{ \vec{x} : \Lambda_{\vec{x}}(H_0; H_1) > k \} \text{ for some $k$ }\mpunct{.}
\]

\begin{theorem}[name=Neyman-Pearson lemma,label=thm:np]
Suppose $H_0 : f = f_0$ versus $H_1 : f = f_1$ where $f_0$ and $f_1$ are continuous \pdfs that are positive on the same region.
Then, among all tests of size at most $\alpha$, the test with the smallest probability of a Type II error (i.e. the most powerful test) is the LR-test with critical region:
\[
C = \left\{ \vec{x} : \frac{f_1(\vec{x})}{f_0(\vec{x})} > k \right\} \mpunct{,}
\]
where $k$ is chosen such that
\[
\int_C f_0(\vec{x}) d\vec{x} = \PP \left(\vec{X} \in C \given H_0\right) = \alpha \mpunct{.}
\]
\end{theorem}

\begin{proof}
  Let $\beta = \PP (\vec{X} \not\in C \given H_1) = \int_{\overline{C}} f_1(\vec{x}) d\vec{x}$.
Let $C^*$ be the critical region for any other test with size at most $\alpha$,
and let $\alpha^* = \PP (\vec{X} \in C^*  \given H_0)$,
$\beta^* = \PP (\vec{X} \in \overline{C^*} \given H_1)$.
We want to show $\beta \leq \beta^*$.

By assumption, we have $\alpha^* \leq \alpha$, i.e.
\[
\int_{C^*} f_0(\vec{x})d\vec{x} \leq \int_C f_0(\vec{x})d\vec{x} \mpunct{.}
\]
Also, for $\vec{x} \in C$ (resp. $\vec{x} \not\in C$), we have that $f_1(\vec{x}) > k f_0(\vec{x})$ (resp. $f_1(\vec{x}) \leq k f_0(\vec{x})$).
Hence, we have that:
\begin{IEEEeqnarray*}{rCl}
\beta - \beta^* &=& \int_{\overline{C}} f_1(\vec{x})d\vec{x} - \int_{\overline{C^*}} f_1(\vec{x})d\vec{x} \\
&=& \int_{\overline{C} \cap C^*} f_1(\vec{x})d\vec{x} + \int_{\overline{C} \cap \overline{C^*}} f_1(\vec{x})d\vec{x} - \int_{\overline{C^*} \cap C} f_1(\vec{x})d\vec{x} - \int_{\overline{C^*} \cap \overline{C}} f_1(\vec{x})d\vec{x} \\
&\leq& k \int_{\overline{C} \cap C^*} f_0(\vec{x})d\vec{x} - k \int_{\overline{C^*} \cap C}f_0(\vec{x})d\vec{x} \\
&=& k \left\{ \left[ \int_{\overline{C} \cap C^*} f_0(\vec{x})d\vec{x} + \int_{C \cap C^*}  f_0(\vec{x})d\vec{x} \right] - \left[ \int_{\overline{C^*} \cap C}  f_0(\vec{x})d\vec{x} + \int_{C \cap C^*}  f_0(\vec{x})d\vec{x}\right] \right\} \\
&=& k \left( \int_{C^*} f_0(\vec{x})d\vec{x} - \int_C f_0(\vec{x})d\vec{x} \right) \\
&=& k \left(\alpha^* - \alpha\right) \leq 0 \mpunct{.}
\end{IEEEeqnarray*}
\end{proof}

In practice, we fix $\PP(\text{Type I error})$ to be $\alpha$ say, and we use \vref{thm:np} to find the ``best'' (i.e. the most powerful) test among tests of size $\leq \alpha$.
$H_0$ and $H_1$ are not treated symmetrically. $H_0$ is called the null hypothesis\index{null hypothesis} and is often conservative (e.g. one of ``no change'', ``no association''\dots).
$H_1$ is called the alternative hypothesis\index{alternative hypothesis} and usually represents the kind of departure from $H_0$ that is of interest.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "statistics"
%%% End:
