$T = \sum \frac{(o_i - e_i)^2}{e_i}$ is Pearson's chi-squared statistic\index{Pearson's chi-squared statistic} and reject $H_0$ if $T > \chi^2_{k-1-\abs{\Theta}}(\alpha$.

\begin{example}[name=Mendel's peas, label=ex:2.4]
We consider an experiment whereby we cross $556$ smooth yellow peas with wrinked green female peas. From the progeny, we let
\begin{itemize}
\item $N_1$ be the number of smooth yellow peas,
\item $N_2$ be the number of smooth green peas,
\item $N_3$ be the number of wrinkled yellow peas,
\item $N_4$ be the number of wrinkled green peas.
\end{itemize}
We test the goodness of fit test of the model:
\[
H_0 : (p_1, p_2, p_3, p_4) = (9/16, 3/16, 3/16, 1/16) \mpunct{.}
\]
Observe that $(n_1, n_2, n_3, n_4) = (315, 108, 102, 31)$. \\
We find that $(e_1, e_2, e_3, e_4) = (312.75, 104.25, 104.25, 34.75)$, hence we have that
\[
2 \log \Lambda = 0.618 \text{ and } \sum \frac{(o_i - e_i)^2}{e_i} = 0.604 \mpunct{.}
\]

Now, we have that $\abs{\Theta_0} = 0$ and $\abs{\Theta_1} = 4 - 1 = 3$, hence we must refer our test statistic to $\chi^2_3$.
We have that $\chi^2_3 (0.05) = 7.815$, so neither value is significant at the $5\%$ level, and there is no evidence against Mendel's theory.
In fact, the $p$-value is approximately $\PP(\chi^2_3 > 0.6) \approx 0.9$, the values are ``very ordinary''.
\end{example}

\section{Testing independence in contingency tables\label{sec:2.6}}
Contingency tables\index{contingency table} arise when observations or individuals are classified according to two or more criteria.

\begin{example}[label=ex:2.5]
$500$ people with recent car changes were asked about their previous and new cars.
The results of the poll are presented in \vref{table:ex:2.5}.
This is a two-way contingency table: each person is classified according to previous car size and new car size.

\begin{table}[h]
  \centering
  \begin{tabular}{rrrr}
    \toprule
    & \multicolumn{3}{c}{New car} \\
    \cmidrule{2-4}
    Previous car & Large & Medium & Small \\
    \midrule
    Large & $56$ & $52$ & $42$ \\
    Medium & $50$ & $83$ & $67$ \\
    Small & $18$ & $51$ & $81$ \\
    \bottomrule
  \end{tabular}
  \caption{Number of people per old and new car size.}
  \label{table:ex:2.5}
\end{table}
\end{example}

Consider a two-way contingency table with $r$ rows and $c$ columns.
Let $p_{ij}$ be the probability that a randomly selected individual is classified in row $i$ and column $j$, i.e. is in the $(i, j)$ cell of the table.
Let
\begin{IEEEeqnarray*}{rCl}
p_{i+} ( = p_{i\cdot} ) &=& \sum_{j=1}^c p_{ij} = \PP(\text{in row $i$}) \mpunct{,} \\
p_{+j} ( = p_{\cdot j} ) &=& \sum_{i = 1}^r p_{ij} = \PP(\text{in column $j$}) \mpunct{.}
\end{IEEEeqnarray*}
We must have that $p_{++} = \sum_{i, j} p_{ij} = 1$, i.e. $\sum_i p_{i+} = 1 = \sum_j p_{+j}$.

Suppose a random sample of $n$ individuals is taken.
Let $N_{ij}$ be the number of these in the $(i, j)$ cell.
Let $N_{i+} = \sum_j N_{ij}$, $N_{+j} = \sum_i N_{ij}$, $N_{++} = \sum_{i, j} N_{ij}$.
Then, we have that
\[
(N_{11}, N_{12}, \dotsc, N_{1c}, N_{21}, \dotsc, N_{2c}, \dotsc, N_{rc}) \sim \Multinomial (n; p_{11}, p_{12}, \dotsc, p_{1c}, p_{21}, \dotsc, p_{2c}, \dotsc, p_{rc}) \mpunct{.}
\]

We test $H_0 : \text{the two classifications are independent}$, i.e.
\[
H_0 : p_{ij} = p_{i+}p_{+j} \text{ for } i = 1, \dotsc, r \quad j = 1, \dotsc, c
\]
with constraints $sum_i p_{i+} = 1$ and $\sum_j p_{+j} = 1$,
versus $H_1 : p_{ij} \text{ unrestricted, except for the usual normalization conditions}$.

Under $H_1$, the maximum likelihood estimators are $\hat{p_{ij}} = n_{ij}/n$ (as obtained in \ref{sec:2.5}.

Under $H_0$, we have that $L \propto \prod (p_{i+}p_{+j})^{n_{ij}}$. Use Lagrangian methods with:
\[
\mathscr{L} = \sum_i n_{i+} \log p_{i+} + \sum_j n_{+j} \log p_{+j} - \lambda\left(\sum p_i - 1\right) - \mu\left(\sum p_{+j} - 1\right) \mpunct{.}
\]
We the find that $\hat{p_{i+}} = n_{i+}/n$ and $\hat{p_{+j}} = n_{+j}/n$.

Let $o_{ij} = n_{ij}$, $e_{ij} = n\hat{p_{i+}}\hat{p_{+j}} = n_{i+}n_{+j}/n$.
Now we have that
\begin{IEEEeqnarray*}{rCl}
2 \log \Lambda &=& 2 \sum_{i = 1}^r \sum_{j=1}^c o_{ij} \log \left(\frac{o_{ij}}{e_{ij}}\right) \\
&\approx& T = \sum_{i = 1}^r \sum_{j=1}^c \frac{(o_{ij} - e_{ij})^2}{e_{ij}} \mpunct{.}
\end{IEEEeqnarray*}
We have that $\abs{\Theta} = rc - 1$, and $\abs{\Theta_0} = (r-1) + (c-1)$, hence $\abs{\Theta} - \abs{\Theta_0} = rc - 1 - (r - 1) - (c - 1) = (r-1)(c-1)$.
Hence $2 \log \Lambda$ and $T$ are compared to $\chi^2_{(r-c)(c-1)} (\alpha)$.

\begin{example}[continues=ex:2.5]
Now suppose that we wish to test $H_0$, the hypothesis that the new and previous car sizes are independent.

\begin{table}[h]
  \centering
  \begin{tabular}{rrrr}
    \toprule
    & \multicolumn{3}{c}{New car} \\
    \cmidrule{2-4}
    Previous car & Large & Medium & Small \\
    \midrule
    Large & 37.2 & 55.8 & 57.0 \\
    Medium & 49.6 & 74.4 & 76.0 \\
    Small & 37.2 & 55.8 & 57.0 \\
    \bottomrule
  \end{tabular}
  \caption{Expected values for car ownership under $H_0$}
  \label{tab:ex:2.5.2}
\end{table}

We obtain, using the previous formula (and the values of $e_{ij}$ as in \vref{tab:ex:2.5.2})
\[
\sum_{i, j} \frac{(o_{ij}-e_{ij})^2}{e_{ij}} = 36.20 \mpunct{.}
\]
We have $(3-1)(3-1)=4$ degrees of freedom, hence we find that $\chi^2_4 (0.05) = 9.488$ and $\chi^2_4 (0.01) = 13.28$.
The observed value of $36.20$ is significant at the $1\%$ level, hence we have strong evidence against $H_0$.
We can thus conclude that the new and present car sizes are not independent.

\begin{table}[h]
  \centering
  \begin{tabular}{rSSS}
    \toprule
    & \multicolumn{3}{c}{New car} \\
    \cmidrule{2-4}
    Previous car & {Large} & {Medium} & {Small} \\
    \midrule
    Large & 3.08 & -0.51 & -1.99 \\
    Medium & 0.06 & 1.00 & -1.03 \\
    Small & -3.14 & -0.64 & 3.18 \\
    \bottomrule
  \end{tabular}
  \caption{Deviation from expected value for car ownership}
  \label{tab:ex:2.5.3}
\end{table}

In fact, we can consider the values of $(o_{ij} - e_{ij})/e_{ij}$ in each cell (as in \vref{tab:ex:2.5.3}), and we see that:
\begin{itemize}
\item more owners of large cars than expected under $H_0$ bought another large car,
\item more owners of small cars than expected under $H_0$ bought another small car,
\item fewer than expected under $H_0$ changed from a small to a large car.
\end{itemize}
\end{example}

\section{Testing homogeneity in contingency tables}
\label{sec:2.7}

\begin{example}
\begin{table}[h]
  \centering
  \begin{tabular}{rrrr}
    \toprule
    & Improved & No difference & Worse \\
    \midrule
    Placebo & 18 & 17 & 15 \\
    Half dose & 20 & 10 & 20 \\
    Full dose & 25 & 13 & 12 \\
    \bottomrule
  \end{tabular}
  \caption{Results of the experiment}
  \label{tab:ex:2.6}
\end{table}

$150$ patients are randomly divided into three groups of $50$ patients. The experiment is carried out, and the result are reported in \vref{tab:ex:2.6}.
Let $H_0$ be the hypothesis that ``the probability of `improved' is the same for each of the three treatment groups, and so are the probabilities of `no difference' and `worse' ''.
$H_0$ is the hypothesis that we have homogeneity between the rows.

\emph{the carrying out of the test is left as an exercise to the reader.}
\end{example}

In general, we have independent observations from $r$ multinomial distributions, each with $c$ categories.
\[
(N_{i1}, \dotsc, N_{ic}) \sim \Multinomial(n_{i+}, p_{i1}, \dotsc, p_{ic}) \mpunct{.}
\]
We test the following hypothesis
\[
H_0 : p_{1j} = p_{2j} = \dotsc = p_{rj} ( = p_j \text{, say}), \ j = 1, \dotsc, c
\]
with the constraint that $\sum_j p_j = 1$, against the hypothesis:
\[
H_1 : p_{ij} \text{ unrestricted}
\]
with the constraint $p_{i+} = 1$ for $i = 1, \dotsc, r$.

Using a generalized likelihood ratio test, we find that
\[
2 \log \Lambda = 2 \sum_{i = 1}^r \sum_{j = 1}^c n_{ij} \log \left( \frac{n n_{ij}}{n_{i+}n_{+j}} \right) \mpunct{,}
\]
which is identical to the $2 \log \Lambda$ as in \cref{sec:2.6}, and similarly for $T$.
Here, we have that $\abs{\Theta_0} = c-1$, $\abs{\Theta} = r(c-1)$, so that $\abs{\Theta} - \abs{\Theta_0} = (r-1)(c-1)$.
Hence, both $2 \log \Lambda$ and $T$ are referred to $\chi^2_{(r-1)(c-1)} (\alpha)$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "statistics"
%%% End:
