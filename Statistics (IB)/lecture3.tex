Minimal sufficient statistics can be found using the following theorem (\ref{thm:1.2}).

\begin{theorem}
  \label{thm:1.2}

  Suppose that there exists a function $T(\vec{x})$ such that:
\[
\frac{f_{\vec{x}}(\vec{x}; \theta)}{f_{\vec{x}}(\vec{y}, \theta)} \text{ is constant as a function of $\theta$} \Leftrightarrow T(\vec{x}) = T(\vec{y})
\]
then $T(\vec{x})$ is minimal sufficient for $\theta$.
\end{theorem}

\begin{proof}
  \emph{see handout}
\end{proof}

\paragraph{Example}
Let $X_1, \dotsc, X_n$ be \iid $\Normal(\mu, \sigma^2)$, and let $\theta = (\mu, \sigma^2)$. We have the following:
\begin{IEEEeqnarray*}{rCl}
\frac{f_{\vec{x}}(\vec{x}, \mu, \sigma^2)}{f_{\vec{x}}(\vec{y}, \mu, \sigma^2)} &=& 
\frac{\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n\exp\left\{-\frac{1}{2\sigma^2}\sum (x_i - \mu)^2\right\}}{\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left\{-\frac{1}{2\sigma^2}\sum (y_i - \mu)^2\right\}} \\
&=& \exp\left\{ -\frac{1}{2\sigma^2}\left(\sum x_i^2 - \sum y_i^2\right) + \frac{\mu}{\sigma^2}\left(\sum x_i - \sum y_i\right)\right\} \mpunct{.}
\end{IEEEeqnarray*}
This ratio is constant as a function of $(\mu, \sigma^2)$ if and only if $\sum x_i^2 = \sum y_i^2$ and $\sum x_i = \sum y_i$. Hence by \cref{thm:1.2}, we have $T(\vec{X}) = \left(\sum X_i, \sum X_i^2\right)$ is minimal sufficient for $(\mu, \sigma^2)$.

\paragraph{Notes}

\begin{enumerate}
\item Bijective functions of minimal sufficient statistics are minimal sufficient (as they give rise to the same partition of $\curlyX^n$), so $T'(\vec{X}) = \left(\overline{X}, \frac{1}{n}\sum(X_i - \overline{X})^2\right)$ is also minimal sufficient in the previous example, where $\overline{X} = \frac{1}{n}\sum X_i$.
\item The previous example has a vector $T$ sufficient for a vector $\theta$. In general, dimensions do not have to be the same. For example, consider $\Normal(\mu, \mu^2)$, we can find that $T = \left(\sum X_i, \sum X_i^2\right)$ is minimal sufficient for $\mu$.
\item If the range of $X$ depends on $\theta$, then the statement ``$\frac{f_{\vec{x}}(\vec{x}, \theta)}{f_{\vec{x}}(\vec{x}, \theta)}$ is constant as a function of $\theta$'' means ``$f_{\vec{x}} (\vec{x}, \theta) = c(\vec{x}, \vec{y})f_{\vec{x}}(\vec{y}, \theta)$''. In particular, the numerator and denominator in the ratio must be positive fr exactly the same values of $\theta$.
\end{enumerate}

\section{Maximum likelihood estimation}
Let $X_1, \dotsc, X_n$ be random variables with joint \pdf or \pmf $f_{\vec{X}}(\vec{x}, \theta)$.

\begin{definition}
  The likelihood function is $L(\theta) = f_{\vec{X}}(\vec{x}, \theta)$ regarded as a function of $\theta$.
\end{definition}

We sometimes write $L(\theta; \vec{x})$. We may regard the likelihood as a random variable $L(\theta; \vec{X})$.

\begin{definition}
  Given $\vec{X} = \vec{x}$, a maximum likelihood estimate\index{maximum likelihood estimate} of $\theta$ is a parameter value $\hat{\theta}(\vec{x})$ which maximises $L(\theta; \vec{x})$.

  The maximum likelihood estimator\index{maximum likelihood estimator} (MLE) is $\hat{\theta}(\vec{X})$.
\end{definition}

It is often easier to maximise the log-likelihood:
\[
\ell(\theta) = \log L(\theta) \mpunct{.}
\]

\paragraph{Examples}
\begin{itemize}
\item Let $X_1, \dotsc, X_n$ be \iid $\Bernouilli(p)$. Then the likelihood and log-likelihood functions are:
\[
L(p = p^{\sum x_i}(1-p)^{n - \sum x_i} \text{ and } 
\ell(p) = \left(\sum x_i\right)\log p + \left(n - \sum x_i\right)\log (1 - p) \mpunct{.}
\]
Taking the derivative, we have that:
\[
\frac{\partial \ell}{\partial p} = \frac{\sum x_i}{p} = \frac{n - \sum x_i}{1-p} = \frac{n}{p(1-p)}(\overline{x} - p) \mpunct{.}
\]

\item Let $X_1, \dotsc, X_n$ be \iid $\Normal(\mu, \sigma^2)$, and let $\theta = (\mu, \sigma^2)$. The log-likelihood function is:
\[
\ell (\mu, \sigma^2) = -\frac{n}{2}\log \left(2\pi\right) - \frac{n}{2}\log \left(\sigma^2\right) - \frac{1}{2\sigma^2}\sum (x_i - \mu)^2 \mpunct{.}
\]
We then have the following derivatives:
\[
\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum(x_i - \mu) \quad \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum (x_i - \mu)^2 \mpunct{.}
\]
Solving the simultaneous equations:
\[
\frac{\partial \ell}{\partial \mu} = 0 \quad \frac{\partial \ell}{\partial \sigma^2} = 0
\]
gives $\hat{\mu} = \overline{X}$, and $\hat{\sigma^2} = \frac{1}{n}\sum\left(X_i - \overline{X}\right)^2$. We can check that this is indeed a maximum. 
Hence, the maximum likelihood estimator of $(\mu, \sigma^2)$ is:
\[
\left(\hat{\mu}, \hat{\sigma^2}\right) = \left(\overline{X}, \frac{1}{n}\sum\left(X_i - \overline{X}\right)^2\right) \mpunct{.}
\]

\item Let $X_i, \dotsc, X_n$ be \iid $\Uniform[0, \theta]$. The likelihood function is $L(\theta) = -\frac{1}{\theta^n}\indicator{\max_i x_i \leq \theta}\indicator{\min_i x_i \geq 0}$ (as in previous example). For $\theta \geq \max x_i$, we have $L(\theta) = \frac{1}{\theta^n}$ and is decreasing as $\theta$ increases. For $\theta < \max x_i$, $L(\theta) = 0$.
[Graph of L(theta)]
From the graph, the \mle is $\hat{\theta} = \max_i X_i$.

Is $\hat{\theta}$ unbiased? First we need to find the distribution of $\max X_i$. We do this via the distribution function of $\max X_i$. For $0 \leq t \leq \theta$, we have:
\begin{IEEEeqnarray*}{rCl}
F_\theta(t) = \PP\left(\hat{\theta} \leq t\right) = \PP( \max X_i \leq t) &=& \PP (X_i  \leq t, i = 1, \dotsc, n) \\
&=& \left(\PP(X_1 \leq t)\right)^n \text{ as the $X_i$ are \iid } \\
&=& \left(\frac{t}{\theta}\right)^n
\end{IEEEeqnarray*}
Differentiate with respect to $t$ to find the \pdf:
\[
f_{\hat{\theta}}(t) = \begin{cases} \frac{nt^{n-1}}{\theta^n} & 0 \leq t \leq \theta \\ 0 & \text{otherwise} \end{cases}
\]
so $\EE_\theta[\hat{\theta}] = \int_0^\theta t\frac{nt^n-1}{\theta^n} dt = \frac{n\theta}{n+1}$. So $\hat{\theta}$ is biased, but it is asymptotically unbiased\index{asymptotically unbiased} as $n \rightarrow \infty$.
\end{itemize}

\paragraph{Properties of \mle}
\begin{enumerate}
\item If $T$ is sufficient for $\theta$, then we have:
\[
L(\theta) =  g\left(T(\vec{x}), \theta\right) h(\vec{x}) \mpunct{.}
\]
To maximise $L(\theta)$ a function of $\theta$, we only need to maximise $g\left(T(\vec{x}), \theta\right)$, and so $\theta$ will be a function of the sufficient statistic $T$.
\end{enumerate}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "statistics"
%%% End: 
