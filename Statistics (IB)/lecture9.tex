\paragraph{Example 2.3}
Single normal sample, testing a given mean, known variance: Z-test.

Let $X_1, \dotsc, X_n$ be \iid $\Normal(\mu, \sigma_0^2)$, with $\sigma_0^2$ known.
Let $H_0 : \mu = \mu_0$ and $H_1 : \mu \neq \mu_0$ with $\mu_0$ known.
Then, we have that $\Theta_0  = \{ \mu_0 \}$, hence we have that
\[
\sup_{\mu \in \Theta_0} L(\mu; \vec{x}) = L(\mu_0, \vec{x}) = f(\vec{x}; \mu_0) \mpunct{.}
\]
On the other hand, we have that
\[
\sup_{\mu \in \Theta} L(\mu; \vec{x}) = \sup_{\mu \in \RR} L(\mu; \vec{x}) = L(\hat{\mu}; \vec{x}) = f(\vec{x}; \hat{\mu}) \mpunct{,}
\]
where $\hat{\mu}$ is the maximum likelihood estimator, i.e. $\hat{\mu} = \overline{x}$.
Hence, we have that
\[
\Lambda_{\vec{x}} (H_0; H_1) = \frac{\left( \frac{1}{\sqrt{2 \pi \sigma_0^2}} \right)^n \exp \left\{ -\frac{1}{2 \sigma_0^2} \sum (x_i - \hat{\mu})^2 \right\}}{\left( \frac{1}{\sqrt{2 \pi \sigma_0^2}} \right)^n \exp \left\{ -\frac{1}{2 \sigma_0^2} \sum (x_i - \mu_0)^2 \right\}} \mpunct{,}
\]
so we deduce that
\begin{IEEEeqnarray*}{rCl}
2 \log \Lambda_{\vec{x}} (H_0, H_1) &=& \frac{1}{\sigma_0^2} \left[ \sum (x_i - \mu_0)^2 - \sum (x_i - \overline{x})^2 \right] \\
&=& \frac{n \big(\overline{x} - \mu_0\big)^2}{\sigma_0^2} \mpunct{.}
\end{IEEEeqnarray*}

We reject $H_0$ if $\Lambda$ and hence $2 \log \Lambda$ is large, i.e. if $\Lambda \big(\overline{x} - \mu_0\big)^2/\sigma_0^2$ is large, or equivalently if
\[
\frac{\sqrt{n}}{\sigma_0} \abs*{\overline{x} - \mu_0}
\]
is large.
Under $H_0$, we have that $\frac{sqrt{n}}{\sigma_0}\Big(\overline{X} - \mu_0\Big) \sim \Normal(0, 1)$.
Hence we reject $H_0$ if $\abs*{\sqrt{n}\big(\overline{x} - \mu_0\big)/\sigma_0} > Z_{\alpha/2}$ for a size $\alpha$ test.
This is a two-tailed test\index{two-tailed test}.

Note that since $n\Big(\overline{X} - \mu_0\Big)^2/\sigma_0^2 \sim \xi^2_1$ under $H_0$, so the above test is equivalent to rejecting $H_0$ if $n \big(\overline{x} - \mu_0 \big)^2/\sigma_0^2 > \xi^2_1(\alpha)$ (check that $z_{\alpha/2} = \xi^2_1(\alpha)$).

\subparagraph{Wiks' theorem}
It turns out that we can still use generalised likelihood ratio tests even when we cannot find the exact null distribution of the test statistic.
Suppose $\dim \Theta = k$ and that $\Theta_0$ imposes $p$ independent restrictions so that $\dim \Theta_0 = k - p$.
We write $\abs{\Theta} = k$ and $\abs{\Theta_0} = k - p$.
We say $\Theta$ has ``$k$ free parameters'' and $\Theta_0$ has ``$k-p$ free parameters''.

Under regularity conditions, with $\vec{X} = \big(X_1, \dotsc, X_n\big)$ with $X_i$ \iid, we have that if $H_0$ is true, then $2 \log \Lambda_{\vec{x}} (H_0; H_1)$ is approximately distributed as $\xi^2_p$ for large $n$, and if $H_0$ is not true, then $2 \log \Lambda$ tends to be larger (Wiks' theorem).

In example 2.3, we have that $\abs{\Theta} = 1$, and $\abs{\Theta_0} = 0$, so we have that $p = \abs{\Theta} - \abs{\Theta_0}$, and we have seen that $2 \log \Lambda \sim \xi^2_1$.

\section{Goodness-of-fit test}

Suppose we wish to test whether our data comes from a particular parametric family $\{ f_X(\cdot, \theta) \quad \theta \in \Theta \}$.
Partition $\curlyX$ into sets $A_1, \dotsc, A_k$ and let $p_i = \PP (X_i \in A_i)$ and $p_i(\theta) = \int_{A_i} f_X(x, \theta) \: dx$.

Consider testing $H_0 : p_i  = p_i(\theta) \text{ for } i = 1, \dotsc, k \text{ for some } \theta$ versus $H_1 : p_i \geq 0, \sum p_i = 1$.
This is a goodness-of-fit test\index{goodness-of-fit test} and we carry it out using a generalised likelihood ratio test.

Let $N_i = \text{number of observations in } A_i$. Under $H_1$, we have that $(N_1, \dotsc, N_k) \sim \text{Multinomial}(n, p_1, \dotsc, p_k)$, and the likelihood is such that
\[
L(p_1, \dotsc, p_k) \propto p_1^{n_1} \dotsm p_k^{n_k}
\]
and the log-likelihood is
\[
\ell (p_1, \dotsc, p_k) = \text{constant} + \sum_{i=1}^k n_i \log p_i \mpunct{.}
\]
We maximise the log-likelihood subject to $\sum p_i = 1$ by considering the Lagrangian
\[
\mathscr{L} = \sum_{i=1}^k n_i \log p_i - \lambda(\sum p_i - 1) \mpunct{.}
\]
We find the maximum likelihood estimators $\hat{p_i} = n_i/n$.

Under $H_0$, we can find $\hat{\theta}$ (the \mle of $\theta$) by maximising $\sum n_i \log p_i(\theta)$ with respect to $\theta$. Then, we have that
\begin{IEEEeqnarray*}{rCl}
2 \log \Lambda_{\vec{x}} (H_0; H_1) &=& 2 \log \left( \frac{\hat{p_1}^{n_1} \dotsm \hat{p}^n_k}{p_1(\hat{\theta})^{n_1} \dotsm p_k(\hat{\theta})^{n_k}} \right) \\
&=& 2 \sum n_i \log \left( \frac{\hat{p_i}}{p_i(\hat{\theta})} \right) \\
&=& 2 \sum n_i \log \left( \frac{n_i}{n p_i(\theta)} \right) \mpunct{,}
\end{IEEEeqnarray*}
the generalised likelihood ratio test rejects $H_0$ if $2 \log \Lambda > \xi^2_{k - 1 - \abs{\Theta_0}}$.

If we let $o_i = n_i$ (the observed number in $A_i$), $e_i = n p_i(\hat{\theta})$ (the ``expected'' number in $A_i$ under $H_0$) and $\delta_i = o_i - e_i$ so that $o_i = e_i + \delta_i$.
If $H_0$ is true, we expect $\delta_i << e_i$.
Then, we have that
\begin{IEEEeqnarray*}{rCl}
  2 \log \Lambda_{\vec{x}} (H_0; H_1) &=& 2\sum_{i=1}^k o_i \log \left( \frac{o_i}{e_i} \right) \\
&=& 2 \sum (e_i + \delta_i) \log \left( 1 + \frac{\delta_i}{e_i} \right) \\
&=& 2 \sum \left( \delta_i + \frac{\delta_i^2}{e_i} - \frac{\delta_i^2}{2e_i} + o(\delta_i^2) \right) \\
&\approx& \sum \frac{\delta_i^2}{e_i} \\
&=& \sum \frac{(o_i - e_i)^2}{e_i} \mpunct{.}
\end{IEEEeqnarray*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "statistics"
%%% End:
